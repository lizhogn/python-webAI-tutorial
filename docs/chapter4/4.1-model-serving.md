# 4.1 模型服务化

## 📖 概述

本章介绍如何将 AI/机器学习模型部署为 Web 服务，支持在线推理和前后端集成。内容涵盖模型保存、加载、API 封装、推理接口设计等。

## 🧠 什么是模型服务化？
- 将训练好的模型（如 PyTorch、TensorFlow、sklearn）通过 API 提供在线推理能力
- 支持多端调用（Web、App、第三方系统）
- 便于模型迭代、监控和扩展

## 🏗️ 常见模型服务化方案
- **Flask/FastAPI**：轻量级，适合小型/原型项目
- **TensorFlow Serving**：官方高性能推理服务
- **TorchServe**：PyTorch 官方推理服务
- **ONNX Runtime**：跨框架推理
- **自定义微服务**：灵活扩展

## 🚀 FastAPI 封装模型推理 API

### 1. 保存与加载模型
```python
# 以 sklearn 为例
import joblib
# 保存模型
joblib.dump(model, 'model.pkl')
# 加载模型
model = joblib.load('model.pkl')
```

### 2. FastAPI 封装推理接口
```python
from fastapi import FastAPI
import joblib
import numpy as np

app = FastAPI()
model = joblib.load('model.pkl')

@app.post('/predict')
def predict(features: list):
    X = np.array([features])
    y_pred = model.predict(X)
    return {'result': int(y_pred[0])}
```

### 3. 支持多种输入格式
```python
from pydantic import BaseModel
class PredictRequest(BaseModel):
    features: list

@app.post('/predict')
def predict(req: PredictRequest):
    X = np.array([req.features])
    y_pred = model.predict(X)
    return {'result': int(y_pred[0])}
```

## 🛡️ 推理接口设计建议
- 输入/输出结构化（JSON）
- 明确接口文档（OpenAPI）
- 错误处理与异常返回
- 支持批量推理
- 日志与监控

## 🧪 测试与调用
```python
import requests
resp = requests.post('http://localhost:8000/predict', json={'features': [1,2,3,4]})
print(resp.json())
```

## 📚 学习资源
- [FastAPI 官方文档](https://fastapi.tiangolo.com/)
- [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)
- [TorchServe](https://pytorch.org/serve/)
- [ONNX Runtime](https://onnxruntime.ai/)

## 🔍 知识检查
- [ ] 理解模型服务化的意义
- [ ] 能够用 FastAPI 封装推理接口
- [ ] 能够处理输入输出与异常
- [ ] 能够测试和调用模型服务

---

**上一章**：[第三章 前端开发](../chapter3/README.md) | **下一节**：[4.2 异步处理](4.2-async-processing.md) 