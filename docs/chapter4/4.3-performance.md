# 4.3 性能优化

## 📖 概述

本章介绍 AI 服务和 Web API 的性能优化方法，包括模型推理加速、API 并发优化、缓存、负载均衡等，帮助你提升系统吞吐量和响应速度。

## 🚀 推理性能优化

### 1. 模型格式与推理引擎
- 使用 ONNX、TensorRT、OpenVINO 等高性能推理引擎
- 模型量化/裁剪/蒸馏，减少模型体积和计算量

### 2. 批量推理
- 支持一次请求处理多条数据，提升吞吐量
- 适合 GPU/TPU 场景

### 3. 并发与多进程
- FastAPI + Uvicorn/Gunicorn 多进程/多线程
- Python 原生 async/await

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

## 🏗️ API 性能优化

### 1. 缓存
- 内存缓存（如 lru_cache、functools）
- 分布式缓存（如 Redis）

```python
from functools import lru_cache
@lru_cache(maxsize=128)
def get_model():
    ...
```

### 2. 数据库优化
- 使用索引、分页查询
- 连接池配置

### 3. 静态资源与CDN
- 前端静态资源使用 CDN 加速
- 图片/模型文件分离存储

## 🛡️ 负载均衡与高可用
- Nginx/Traefik 反向代理
- 多实例部署，自动扩缩容（K8s、Docker Swarm）
- 健康检查与自动重启

## 🧪 性能监控与压测
- Prometheus + Grafana 监控 API QPS、延迟、错误率
- ab、wrk、locust 等工具压测

```bash
ab -n 1000 -c 50 http://localhost:8000/predict
```

## 🛠️ 性能优化最佳实践
- 只优化瓶颈，避免过早优化
- 监控+压测，数据驱动优化
- 代码、模型、硬件多层次协同

## 📚 学习资源
- [FastAPI 性能优化](https://fastapi.tiangolo.com/advanced/async-performance/)
- [ONNX Runtime 官方文档](https://onnxruntime.ai/)
- [Nginx 官方文档](https://nginx.org/zh/docs/)
- [Prometheus 官方文档](https://prometheus.io/docs/)

## 🔍 知识检查
- [ ] 理解推理加速和批量处理原理
- [ ] 能够配置 API 并发和缓存
- [ ] 能够进行性能监控和压测
- [ ] 理解负载均衡和高可用方案

---

**上一节**：[4.2 异步处理](4.2-async-processing.md) | **下一节**：[4.4 模型管理](4.4-model-management.md) 